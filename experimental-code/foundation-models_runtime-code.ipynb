{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 lstm+全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# 假设数据存储在一个名为 data.csv 的文件中，文件包含 30 个 x 列和 1 个 y 列\n",
    "data = pd.read_csv('data.csv')\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# 数据标准化\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "x_scaled = scaler_x.fit_transform(x)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    \"\"\"\n",
    "    创建时间序列数据窗口\n",
    "    :param input_data: 输入数据\n",
    "    :param target_data: 目标数据\n",
    "    :param seq_length: 时间序列长度\n",
    "    :return: 序列输入和序列目标\n",
    "    \"\"\"\n",
    "    sequences_x = []\n",
    "    sequences_y = []\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        sequences_x.append(input_data[i:i + seq_length])\n",
    "        sequences_y.append(target_data[i + seq_length])\n",
    "    return np.array(sequences_x), np.array(sequences_y)\n",
    "\n",
    "\n",
    "# 时间网络窗口大小\n",
    "time_windows = [10, 15, 20]\n",
    "\n",
    "\n",
    "# 存储不同窗口大小下的结果\n",
    "results = {}\n",
    "\n",
    "\n",
    "for t in time_windows:\n",
    "    sequences_x, sequences_y = create_sequences(x_scaled, y_scaled, t)\n",
    "\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    sequences_x = torch.FloatTensor(sequences_x)\n",
    "    sequences_y = torch.FloatTensor(sequences_y)\n",
    "\n",
    "\n",
    "    # 数据集划分\n",
    "    train_size = int(0.7 * len(sequences_x))\n",
    "    val_size = int(0.2 * len(sequences_x))\n",
    "    test_size = len(sequences_x) - train_size - val_size\n",
    "\n",
    "\n",
    "    train_x = sequences_x[:train_size]\n",
    "    train_y = sequences_y[:train_size]\n",
    "    val_x = sequences_x[train_size:train_size + val_size]\n",
    "    val_y = sequences_y[train_size:train_size + val_size]\n",
    "    test_x = sequences_x[train_size + val_size:]\n",
    "    test_y = sequences_y[train_size + val_size:]\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    val_dataset = TensorDataset(val_x, val_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "    # LSTM 模型定义\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "\n",
    "    # 超参数\n",
    "    input_size = x.shape[1]\n",
    "    hidden_size = 50\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # 在验证集上评估\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Time window {t}, Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "    # 在测试集上评估\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Time window {t}, Test Loss: {test_loss}')\n",
    "\n",
    "\n",
    "    results[t] = {'model': model, 'test_loss': test_loss}\n",
    "\n",
    "\n",
    "# 保存最佳模型（假设使用最小的测试损失作为最佳模型）\n",
    "best_t = min(results, key=lambda x: results[x]['test_loss'])\n",
    "best_model = results[best_t]['model']\n",
    "torch.save(best_model.state_dict(), 'best_lstm_model.pth')\n",
    "\n",
    "\n",
    "# 加载模型进行预测\n",
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 示例预测\n",
    "sample_x = torch.FloatTensor(sequences_x[0].unsqueeze(0))\n",
    "prediction = predict(best_model, sample_x)\n",
    "prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "print(f'Prediction: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 lstm神经网络模型+全连接层（单时间窗口、单输入文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 使用header参数指定第2行作为列名（索引），跳过第1行，读取从第2行开始的数据\n",
    "file='E:/2412毕业论文/0104数据特征/wind250109/0109沪深300股票1/处理2/处理/000001.SZ_平安银行_all2.xlsx'\n",
    "\n",
    "# 假设我们想要跳过名为'Column2'和'Column4'的列\n",
    "skip_column_names = ['所属申万行业名称(2014)_[行业级别]一级行业', '股票收益率1']\n",
    "\n",
    "# 获取所有列名，除了要跳过的那些\n",
    "all_columns = pd.read_excel(file, nrows=0).columns\n",
    "columns_to_use = [col for col in all_columns if col not in skip_column_names]\n",
    "\n",
    "# 读取Excel文件，只使用未被跳过的列\n",
    "data = pd.read_excel(file, usecols=columns_to_use)\n",
    "data = data.iloc[7:,1:]\n",
    "data.head()\n",
    "\n",
    "x = data.iloc[:, 3:11].values\n",
    "y = data.iloc[:, -5].values\n",
    "\n",
    "# 数据标准化\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "x_scaled = scaler_x.fit_transform(x)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    \"\"\"\n",
    "    创建时间序列数据窗口\n",
    "    :param input_data: 输入数据\n",
    "    :param target_data: 目标数据\n",
    "    :param seq_length: 时间序列长度\n",
    "    :return: 序列输入和序列目标\n",
    "    \"\"\"\n",
    "    sequences_x = []\n",
    "    sequences_y = []\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        sequences_x.append(input_data[i:i + seq_length])\n",
    "        sequences_y.append(target_data[i + seq_length])\n",
    "    return np.array(sequences_x), np.array(sequences_y)\n",
    "\n",
    "\n",
    "# 时间网络窗口大小\n",
    "time_windows = 10\n",
    "\n",
    "sequences_x, sequences_y = create_sequences(x_scaled, y_scaled, time_windows)\n",
    "\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "sequences_x = torch.FloatTensor(sequences_x)\n",
    "sequences_y = torch.FloatTensor(sequences_y)\n",
    "\n",
    "\n",
    "# 数据集划分\n",
    "train_size = int(0.7 * len(sequences_x))\n",
    "val_size = int(0.2 * len(sequences_x))\n",
    "test_size = len(sequences_x) - train_size - val_size\n",
    "\n",
    "\n",
    "train_x = sequences_x[:train_size]\n",
    "train_y = sequences_y[:train_size]\n",
    "val_x = sequences_x[train_size:train_size + val_size]\n",
    "val_y = sequences_y[train_size:train_size + val_size]\n",
    "test_x = sequences_x[train_size + val_size:]\n",
    "test_y = sequences_y[train_size + val_size:]\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "val_dataset = TensorDataset(val_x, val_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# LSTM 模型定义\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# 超参数\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # 在验证集上评估\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Time window {time_windows}, Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}')\n",
    "\n",
    "# 在测试集上评估\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Time window {time_windows}, Test Loss: {test_loss}')\n",
    "torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "\n",
    "# 加载模型进行预测\n",
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 示例预测\n",
    "sample_x = torch.FloatTensor(sequences_x[0].unsqueeze(0))\n",
    "prediction = predict(model, sample_x)\n",
    "prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "print(f'Prediction: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.多个基础模型对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# 假设数据存储在一个名为 data.csv 的文件中，文件包含 30 个 x 列和 1 个 y 列\n",
    "data = pd.read_csv('data.csv')\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# 数据标准化\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "x_scaled = scaler_x.fit_transform(x)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    \"\"\"\n",
    "    创建时间序列数据窗口\n",
    "    :param input_data: 输入数据\n",
    "    :param target_data: 目标数据\n",
    "    :param seq_length: 时间序列长度\n",
    "    :return: 序列输入和序列目标\n",
    "    \"\"\"\n",
    "    sequences_x = []\n",
    "    sequences_y = []\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        sequences_x.append(input_data[i:i + seq_length])\n",
    "        sequences_y.append(target_data[i + seq_length])\n",
    "    return np.array(sequences_x), np.array(sequences_y)\n",
    "\n",
    "\n",
    "# 时间网络窗口大小\n",
    "time_windows = [10, 15, 20]\n",
    "\n",
    "\n",
    "# 存储不同窗口大小下的结果\n",
    "results = {}\n",
    "\n",
    "\n",
    "for t in time_windows:\n",
    "    sequences_x, sequences_y = create_sequences(x_scaled, y_scaled, t)\n",
    "\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    sequences_x = torch.FloatTensor(sequences_x)\n",
    "    sequences_y = torch.FloatTensor(sequences_y)\n",
    "\n",
    "\n",
    "    # 数据集划分\n",
    "    train_size = int(0.7 * len(sequences_x))\n",
    "    val_size = int(0.2 * len(sequences_x))\n",
    "    test_size = len(sequences_x) - train_size - val_size\n",
    "\n",
    "\n",
    "    train_x = sequences_x[:train_size]\n",
    "    train_y = sequences_y[:train_size]\n",
    "    val_x = sequences_x[train_size:train_size + val_size]\n",
    "    val_y = sequences_y[train_size:train_size + val_size]\n",
    "    test_x = sequences_x[train_size + val_size:]\n",
    "    test_y = sequences_y[train_size + val_size:]\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    val_dataset = TensorDataset(val_x, val_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "    # LSTM 模型定义\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "\n",
    "    # GRU 模型定义\n",
    "    class GRUModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(GRUModel, self).__init__()\n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.gru(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "\n",
    "    # Bi-LSTM 模型定义\n",
    "    class BiLSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(BiLSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "            self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "\n",
    "    # Bi-GRU 模型定义\n",
    "    class BiGRUModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(BiGRUModel, self).__init__()\n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "            self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out, _ = self.gru(x)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "\n",
    "    # CNN-LSTM-Attention 模型定义\n",
    "    class CNNLSTMAttentionModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(CNNLSTMAttentionModel, self).__init__()\n",
    "            self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.attention = nn.Linear(hidden_size, 1)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # 调整输入形状以适应 CNN 层\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out = torch.relu(self.conv(x))\n",
    "            out = out.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(out)\n",
    "            attention_weights = torch.softmax(self.attention(out), dim=1)\n",
    "            out = torch.sum(out * attention_weights, dim=1)\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "\n",
    "    # 超参数\n",
    "    input_size = x.shape[1]\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "\n",
    "    models = {\n",
    "        'LSTM': LSTMModel(input_size, hidden_size, num_layers, output_size),\n",
    "        'GRU': GRUModel(input_size, hidden_size, num_layers, output_size),\n",
    "        'Bi-LSTM': BiLSTMModel(input_size, hidden_size // 2, num_layers, output_size),\n",
    "        'Bi-GRU': BiGRUModel(input_size, hidden_size // 2, num_layers, output_size),\n",
    "        'CNN-LSTM-Attention': CNNLSTMAttentionModel(input_size, hidden_size, num_layers, output_size)\n",
    "    }\n",
    "\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f'Training {model_name} with time window {t}')\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "        # 训练模型\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            # 在验证集上评估\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, targets).item()\n",
    "            val_loss /= len(val_loader)\n",
    "            print(f'{model_name}, Time window {t}, Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "        # 在测试集上评估\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, targets).item()\n",
    "        test_loss /= len(test_loader)\n",
    "        print(f'{model_name}, Time window {t}, Test Loss: {test_loss}')\n",
    "\n",
    "\n",
    "        results[(model_name, t)] = {'model': model, 'test_loss': test_loss}\n",
    "\n",
    "\n",
    "# 保存最佳模型（假设使用最小的测试损失作为最佳模型）\n",
    "best_result = min(results.items(), key=lambda x: x[1]['test_loss'])\n",
    "best_model_name, best_t = best_result[0]\n",
    "best_model = best_result[1]['model']\n",
    "torch.save(best_model.state_dict(), 'best_model.pth')\n",
    "\n",
    "\n",
    "# 加载模型进行预测\n",
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 示例预测\n",
    "sample_x = torch.FloatTensor(sequences_x[0].unsqueeze(0))\n",
    "prediction = predict(best_model, sample_x)\n",
    "prediction = scaler_y.inverse_transform(prediction.numpy())\n",
    "print(f'Prediction: {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
